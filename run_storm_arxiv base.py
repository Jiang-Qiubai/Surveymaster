import os
import re
import argparse
import logging
import arxiv
import dspy
import sys
# å¼•å…¥ sentence_transformers ä»¥ä¾¿æˆ‘ä»¬åœ¨ patch ä¸­ä½¿ç”¨
from sentence_transformers import SentenceTransformer

# --- 1. LiteLLM é™éŸ³é…ç½® (é˜²æ­¢çº¢è‰²æ—¥å¿—åˆ·å±) ---
os.environ["LITELLM_LOG"] = "ERROR" 
import litellm
litellm.suppress_debug_info = True
litellm.set_verbose = False
litellm.drop_params = True

from knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs
from knowledge_storm.lm import LitellmModel
from knowledge_storm.utils import load_api_key

# å¼•å…¥éœ€è¦ Monkey Patch çš„æ¨¡å—
import knowledge_storm.storm_wiki.modules.persona_generator as pg_module
import knowledge_storm.storm_wiki.modules.storm_dataclass as storm_dataclass # <--- æ–°å¢å¼•å…¥
import knowledge_storm.storm_wiki.modules.outline_generation as outline_gen_module
import knowledge_storm.storm_wiki.modules.article_generation as article_gen_module

# ==============================================================================
# 0. ç¡¬ç¼–ç é…ç½®åŒºåŸŸ
# ==============================================================================
LLM_API_URL = "http://172.17.0.1:8091/v1"
LLM_MODEL_NAME = "openai/qwen3-32b"
CONTEXT_WINDOW = 32768
# æœ¬åœ° Embedding æ¨¡å‹è·¯å¾„ (æ”¯æŒ ~ å±•å¼€)
LOCAL_EMBEDDING_PATH = "~/models/paraphrase-MiniLM-L6-v2"

# ==============================================================================
# 2. Monkey Patch åŒºåŸŸ (æ ¸å¿ƒä¿®å¤)
# ==============================================================================

# --- Patch 1: å±è”½ Wikipedia ---
def bypass_wiki_access(url):
    return "Academic Topic Placeholder", "Content skipped: Local Academic Mode."

pg_module.get_wiki_page_title_and_toc = bypass_wiki_access

# --- Patch 2: å¼ºåˆ¶ä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹ (ä¿®å¤ç½‘ç»œè¶…æ—¶) ---
def local_prepare_table_for_retrieval(self):
    # å±•å¼€ç”¨æˆ·ç›®å½• ~
    model_path = os.path.expanduser(LOCAL_EMBEDDING_PATH)
    print(f"ğŸ§  [Embedding] Loading local model from: {model_path}")
    
    if not os.path.exists(model_path):
        print(f"âŒ Error: Local embedding path not found: {model_path}")
        # å¦‚æœæ‰¾ä¸åˆ°æœ¬åœ°æ¨¡å‹ï¼Œä½ å¯ä»¥é€‰æ‹©æŠ›å‡ºå¼‚å¸¸æˆ–å›é€€
        raise FileNotFoundError(f"Local embedding model not found at {model_path}")

    # å…³é”®ä¿®æ”¹ï¼šä¼ å…¥æœ¬åœ°è·¯å¾„ï¼Œå¹¶å¼ºåˆ¶ local_files_only=True
    self.encoder = SentenceTransformer(model_path, device="cpu", local_files_only=True)
    
    # åŸæœ‰é€»è¾‘ä¿æŒä¸å˜
    self.collected_urls = []
    self.collected_snippets = []
    for url, information in self.url_to_info.items():
        for snippet in information.snippets:
            self.collected_urls.append(url)
            self.collected_snippets.append(snippet)
    
    # ç¼–ç æ—¶å…³é—­è¿›åº¦æ¡ï¼Œå‡å°‘æ—¥å¿—å¹²æ‰°
    self.encoded_snippets = self.encoder.encode(self.collected_snippets, show_progress_bar=False)

# åº”ç”¨ Patch è¦†ç›–åŸæ–¹æ³•
storm_dataclass.StormInformationTable.prepare_table_for_retrieval = local_prepare_table_for_retrieval


# ==============================================================================
# 1. è‡ªå®šä¹‰ LLM ç±» (æ¸…æ´—è¾“å‡º)
# ==============================================================================
class CleanLitellmModel(LitellmModel):
    def __call__(self, prompt, **kwargs):
        kwargs['logger_fn'] = None 
        kwargs['verbose'] = False
        outputs = super().__call__(prompt, **kwargs)
        cleaned_outputs = []
        
        for out in outputs:
            if not isinstance(out, str):
                cleaned_outputs.append(out)
                continue
            
            # æ¸…æ´— <think> å’Œå£è¯­
            cleaned = re.sub(r"<think>.*?</think>", "", out, flags=re.DOTALL).strip()
            cleaned = re.sub(r"^<think>.*", "", cleaned, flags=re.DOTALL).strip()
            cleaned = re.sub(
                r"^(Okay|Sure|Here is|Certainly|Let's|To answer|Great|I can help|Based on).*?[:\n]", 
                "", cleaned, flags=re.IGNORECASE | re.MULTILINE
            ).strip()
            
            json_match = re.search(r"```json(.*?)```", cleaned, re.DOTALL)
            if json_match:
                cleaned = json_match.group(1).strip()
            elif "```" in cleaned:
                code_match = re.search(r"```(.*?)```", cleaned, re.DOTALL)
                if code_match:
                    cleaned = code_match.group(1).strip()

            cleaned_outputs.append(cleaned)
        return cleaned_outputs

# ==============================================================================
# 3. ArXiv æ£€ç´¢æ¨¡å— (é˜²å´©æºƒ)
# ==============================================================================
class ArXivSearch(dspy.Retrieve):
    def __init__(self, k=5, category="cs.SE"):
        super().__init__(k=k)
        self.k = k
        self.category = category
        self.client = arxiv.Client()

    def forward(self, query_or_queries, exclude_urls=[]):
        queries = [query_or_queries] if isinstance(query_or_queries, str) else query_or_queries
        collected_results = []
        
        for query in queries:
            safe_query = query.replace(':', ' ').replace('-', ' ').replace('"', '').strip()
            # ç§»é™¤ query/queries è¿™ç§æ¨¡å‹ç”Ÿæˆçš„å…ƒè¯æ±‡
            safe_query = re.sub(r'\b(query|queries)\b', '', safe_query, flags=re.IGNORECASE).strip()
            
            if not safe_query or '<' in safe_query: continue
                
            search_query = f'{safe_query}'
            if self.category:
                search_query += f' AND cat:{self.category}'
            
            print(f"ğŸ” [ArXiv] Searching: {search_query}")
            
            try:
                search = arxiv.Search(query=search_query, max_results=self.k, sort_by=arxiv.SortCriterion.Relevance)
                results = list(self.client.results(search))
                for r in results:
                    collected_results.append({
                        'url': r.entry_id,
                        'title': r.title.replace('\n', ' '),
                        'description': r.summary.replace('\n', ' '),
                        'snippets': [r.summary.replace('\n', ' ')] 
                    })
            except Exception as e:
                print(f"âš ï¸ ArXiv Search Error: {e}")
        
        if not collected_results:
            print("âš ï¸ [Warning] No results found. Returning placeholder.")
            collected_results.append({
                'url': 'http://placeholder/no-results',
                'title': 'No Academic Papers Found',
                'description': 'Search returned no results.',
                'snippets': ['No relevant information found in ArXiv.']
            })
        return collected_results

# ==============================================================================
# 4. ä¸»ç¨‹åº
# ==============================================================================
def main():
    parser = argparse.ArgumentParser(description="STORM ArXiv Review (Local Embedding Fix)")
    parser.add_argument('--topic', type=str, required=True, help="è®ºæ–‡ç»¼è¿°ä¸»é¢˜")
    parser.add_argument('--output-dir', type=str, default="./results/arxiv_review", help="è¾“å‡ºç›®å½•")
    
    # æœç´¢é…ç½®
    parser.add_argument('--max-conv-turns', type=int, default=3, help="å¯¹è¯è½®æ•°")
    parser.add_argument('--max-search-queries', type=int, default=2, help="æ¯è½®æœ€å¤§æœç´¢æ•°")
    parser.add_argument('--arxiv-category', type=str, default="cs.AI", help="ArXivåˆ†ç±»")
    
    args = parser.parse_args()

    survey_topic = f"A Comprehensive Academic Literature Review on {args.topic}"
    print(f"ğŸš€ å¯åŠ¨ä»»åŠ¡: {survey_topic}")
    print(f"ğŸ¤– è¿æ¥ LLM: {LLM_MODEL_NAME}")

    # åˆå§‹åŒ–æ¨¡å‹
    conv_lm = CleanLitellmModel(
        model=LLM_MODEL_NAME, api_key='EMPTY', api_base=LLM_API_URL, model_type='chat',  
        max_tokens=2048, temperature=0.8, top_p=0.9
    )
    article_lm = CleanLitellmModel(
        model=LLM_MODEL_NAME, api_key='EMPTY', api_base=LLM_API_URL, model_type='chat', 
        max_tokens=8192, temperature=0.5, top_p=0.9
    )

    lm_config = STORMWikiLMConfigs()
    lm_config.set_conv_simulator_lm(conv_lm)
    lm_config.set_question_asker_lm(conv_lm)
    lm_config.set_outline_gen_lm(article_lm)
    lm_config.set_article_gen_lm(article_lm)
    lm_config.set_article_polish_lm(article_lm)

    rm = ArXivSearch(k=5, category=args.arxiv_category)

    runner_args = STORMWikiRunnerArguments(
        output_dir=args.output_dir,
        max_conv_turn=args.max_conv_turns,
        max_search_queries_per_turn=args.max_search_queries,
        retrieve_top_k=5, 
        max_thread_num=4, 
        search_top_k=5
    )

    runner = STORMWikiRunner(runner_args, lm_config, rm)

    # æ³¨å…¥å­¦æœ¯ Prompt
    print("ğŸ¨ æ³¨å…¥å­¦æœ¯ç»¼è¿° Prompt...")
    outline_gen_module.WritePageOutline.__doc__ = """
    Write a comprehensive academic literature review outline for the given topic.
    The outline should be structured logically (e.g., Introduction, Methodology, Key Themes, Discussion, Conclusion).
    Use "#" for section titles and "##" for subsections. Do not include the topic name itself as a section.
    """
    article_gen_module.WriteSection.__doc__ = """
    Write an academic review section based on the collected information.
    Synthesize the findings from the provided papers. Be critical, formal, and objective.
    Use [1], [2], ..., [n] to cite the sources inline.
    Do not include a References section at the end (it will be handled automatically).
    """

    print("ğŸ å¼€å§‹æ‰§è¡Œ STORM æµç¨‹...")
    try:
        runner.run(
            topic=survey_topic,
            do_research=True,
            do_generate_outline=True,
            do_generate_article=True,
            do_polish_article=True
        )
        runner.post_run()
        runner.summary()
        print(f"âœ… ä»»åŠ¡å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {os.path.join(args.output_dir, runner.article_dir_name)}")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # å±è”½æ—¥å¿—
    logging.getLogger("LiteLLM").setLevel(logging.WARNING) 
    logging.getLogger("litellm").setLevel(logging.WARNING) 
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("arxiv").setLevel(logging.WARNING)
    logging.getLogger("dspy").setLevel(logging.WARNING)
    # å±è”½ sentence_transformers çš„ INFO æ—¥å¿—
    logging.getLogger("sentence_transformers").setLevel(logging.WARNING)

    main()